#!/bin/bash
#SBATCH -J test_coolmuc_internode_offload_bench_compute
#SBATCH -D ./
#SBATCH -o ./output/%x.%j.out 
#SBATCH -e ./output/%x.%j.err 
#SBATCH --get-user-env
#SBATCH --clusters=inter
#SBATCH --partition=cm4_inter
##SBATCH --qos=cm4_inter
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
#SBATCH --time=00:30:00

module load slurm_setup
module load openmpi/4.1.7-gcc13

export OMPI_MCA_hwloc_base_verbose=1

# To ignore the shared file system warning with PRTE
# export PRTE_MCA_prte_silence_shared_fs=1

export OMP_PROC_BIND=CLOSE OMP_PLACES=CORES OMP_DISPLAY_AFFINITY=TRUE
#export COMMOFF_THREAD_CPU_ID=0:19,1:39,2:19,3:39
export OMP_DISPLAY_AFFINITY=true
export OMP_NUM_THREADS=19
export PREFIX=/dss/dsshome1/0C/di35hef/mmcso/build_ompi/lib

CSV_NATIVE=./result/test_coolmuc_internode_offload_compute_2nodes_4procs.csv
CSV_HEADER=rank,nthreads,thread,rep,work,msg_size,post_time,wait_time,comp_time,comm_time
echo ${CSV_HEADER} > ${CSV_NATIVE}

bash /dss/dsshome1/0C/di35hef/mmcso/experiments/affinity_setup.sh 4 20 40

BENCH="mpiexec -n 4 /dss/dsshome1/0C/di35hef/mmcso/build_ompi/bench/mt_overlap"

for work in 100; do
	for msg in 1 100; do   
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_OFFLOAD} -r 1000
	done
done

