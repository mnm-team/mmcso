#!/bin/bash
#SBATCH -J internode_offload_bench_compute
#SBATCH -D ./
#SBATCH -o ./output/%x.%j.out 
#SBATCH -e ./output/%x.%j.err 
#SBATCH --get-user-env
#SBATCH --clusters=inter
#SBATCH --partition=cm4_inter
##SBATCH --qos=cm4_inter
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
#SBATCH --time=08:00:00

module load slurm_setup
module load openmpi/4.1.7-gcc13

export OMPI_MCA_hwloc_base_verbose=1

# To ignore the shared file system warning with PRTE
# export PRTE_MCA_prte_silence_shared_fs=1

export OMP_PROC_BIND=CLOSE OMP_PLACES=CORES OMP_DISPLAY_AFFINITY=TRUE
#export COMMOFF_THREAD_CPU_ID=0:19,1:39,2:19,3:39
export OMP_DISPLAY_AFFINITY=true
export OMP_NUM_THREADS=19
export PREFIX=/dss/dsshome1/0C/di35hef/mmcso/build_ompi/lib

CSV_FILE=./result/internode_offload_bench_compute_2nodes_4procs.csv
CSV_HEADER=rank,nthreads,thread,rep,work,msg_size,post_time,wait_time,comp_time,comm_time

bash /dss/dsshome1/0C/di35hef/mmcso/experiments/affinity_setup.sh 4 20 40

BENCH="mpiexec -n 4 /dss/dsshome1/0C/di35hef/mmcso/build_ompi/bench/mt_overlap"

echo ${CSV_HEADER} > ${CSV_FILE}
for work in 100 1000 10000 100000; do
	REP=5000
	for msg in 1 10 100 1000; do
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_FILE} -r ${REP}
	done
	REP=500
	for msg in 10000 100000; do
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_FILE} -r ${REP}
	done
	REP=100
	for msg in 1000000 10000000; do
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_FILE} -r ${REP}
	done
	REP=20
	for msg in 100000000; do
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_FILE} -r ${REP}
	done
done
