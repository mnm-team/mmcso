#!/bin/bash
#SBATCH -J intra_offload_compute_mtovlap
#SBATCH -o ./output/%x.%j.out 
#SBATCH -e ./output/%x.%j.err 
#SBATCH -D ./
#SBATCH --time=10:30:00
#SBATCH --no-requeue
#SBATCH --get-user-env
#SBATCH --account=pn76bu
#SBATCH --partition=micro
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=2

module load slurm_setup
module load gcc
module load cmake

export KMP_AFFINITY=verbose
export I_MPI_DEBUG=4

export OMP_PROC_BIND=CLOSE OMP_PLACES=CORES OMP_DISPLAY_AFFINITY=TRUE
#export COMMOFF_THREAD_CPU_ID=0:23,1:23,2:47,3:47
export OMP_DISPLAY_AFFINITY=true
export OMP_NUM_THREADS=23
export PREFIX=/dss/dsshome1/0C/di35hef/mmcso/build_impi/lib

CSV_OFFLOAD=./result/intra_offload_compute_1node_2procs.csv
CSV_HEADER=rank,nthreads,thread,rep,work,msg_size,post_time,wait_time,comp_time,comm_time
echo ${CSV_HEADER} > ${CSV_OFFLOAD}

bash /dss/dsshome1/0C/di35hef/mmcso/experiments/affinity_setup.sh 2 24 48

BENCH="mpiexec -n 2 /dss/dsshome1/0C/di35hef/mmcso/build_impi/bench/mt_overlap"

for work in 100 1000 10000 100000; do
	for msg in 1 100 1000 10000 100000 1000000 10000000 100000000; do  
		LD_PRELOAD=${PREFIX}/libmmc.so ${BENCH} -m ${msg} -w ${work} -f ${CSV_OFFLOAD} -r 1000
	done
done

